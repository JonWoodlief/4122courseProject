{"status":"OK","data":{"id":60641,"identifier":"DVN/I0FFJV","persistentUrl":"https://doi.org/10.7910/DVN/I0FFJV","protocol":"doi","authority":"10.7910","publisher":"Harvard Dataverse","publicationDate":"2013-05-21","storageIdentifier":"s3://dvn-cloud:1902.1/21527","latestVersion":{"id":62167,"storageIdentifier":"s3://dvn-cloud:1902.1/21527","versionNumber":2,"versionMinorNumber":0,"versionState":"RELEASED","versionNote":"","distributionDate":"2013","productionDate":"Production Date","lastUpdateTime":"2013-05-31T06:06:09Z","releaseTime":"2013-05-31T00:00:00Z","createTime":"2013-05-24T04:46:01Z","license":"CC0","termsOfUse":"CC0 Waiver","metadataBlocks":{"citation":{"displayName":"Citation Metadata","fields":[{"typeName":"title","multiple":false,"typeClass":"primitive","value":"Survey of Income and Program Participation (SIPP)"},{"typeName":"author","multiple":true,"typeClass":"compound","value":[{"authorName":{"typeName":"authorName","multiple":false,"typeClass":"primitive","value":"Damico, Anthony"}}]},{"typeName":"datasetContact","multiple":true,"typeClass":"compound","value":[{"datasetContactEmail":{"typeName":"datasetContactEmail","multiple":false,"typeClass":"primitive","value":"ajdamico@gmail.com"}}]},{"typeName":"dsDescription","multiple":true,"typeClass":"compound","value":[{"dsDescriptionValue":{"typeName":"dsDescriptionValue","multiple":false,"typeClass":"primitive","value":"<h3 class=\"post-title entry-title\" itemprop=\"name\"> analyze the survey of income and program participation (sipp) with r </h3> if the census bureau's budget was gutted and only one complex sample survey survived, pray it's the survey of income and program participation (sipp).  it's giant.  it's rich with variables.  it's monthly.  it follows households over three, four, now five year panels.  the congressional budget office uses it for their <a href=\"http://www.cbo.gov/sites/default/files/cbofi\nles/ftpdocs/87xx/doc8712/10-31-healthinsurmodel.pdf\">health insurance simulation\n</a>.  analysts read that sipp has person-month files, get scurred, and retreat to inferior options.  the <a href=\"http://usgsd.blogspot.com/search/label/american%20community%20survey%20%28acs%29\">american community survey</a> may be the mount everest of survey data, but sipp is most certainly the amazon.  questions swing wild and free through the jungle canopy i mean <a href=\"http://smpbff2.dsd.census.gov/pub/sipp/2008/l08puw1d.txt\">core data dictionary</a>.  legend has it that there are still\nspecies of <a href=\"http://www.census.gov/sipp/top_mod/2008/topmod08.html\">topical module variables</a> that scientists like you have yet to analyze.  <a href=\"http://en.wikipedia.org/wiki/Fountain_of_Youth\">ponce de le√≥n</a> would've loved it here.  ponce.  what a name.  what a guy.<br /> <br /> the sipp 2008 panel data started from a sample of 105,663 individuals in 42,030 households.  once the sample gets drawn, the census bureau surveys one-fourth of the respondents every four months, over f\nour or five years (panel durations vary).  you absolutely must read and understand\n<a href=\"http://www.census.gov/sipp/usrguide/ch2_nov20.pdf\">pdf pages 3, 4, and 5 of this document</a> before starting any analysis (start at the header 'waves and rotation groups').  if you don't comprehend what's going on, try their <a href=\"http://www.census.gov/sipp/tutorial/SIPP_Tutorial_Beta_version/SIPPtutorialHTML/S_group.htm\">survey design tutorial</a>. <br /> <br /> since sipp collects information from respondents regarding every month over the duration of the panel, you'll need to be\nhyper-aware of whether you want your results to be point-in-time, annualized, or specific to some other period.  the analysis scripts below provide examples of each.  at every four-month interview point, every respondent answers\n<a href=\"http://smpbff2.dsd.census.gov/pub/sipp/2008/l08puw1d.txt\">every core question</a> for the previous four months.  after that, <a href=\"http://www.census.gov/sipp/top_mod/top_mods_chart.html\">wave-specific addenda</a> (called topical modules) get asked, but generally only regarding a single prior month.  to repeat: core wave files contain four records per person, topical modules contain one.  if you stacked every core wave, you would have one record per person per month for the duration o\nf the panel.  mmmassive.  ~100,000 respondents x 12 months x ~4 years.  have an analysis plan before you start writing code so you extract exactly what you need, nothing more.  better yet, modify something of mine.  cool?  this new github repository contains eight, you read me, eight scripts:\n<br /> <br /> <b>1996 panel - download and create database.R </b><br /> <b>2001 panel - download and create database.R </b><br /> <b>2004 panel - download and create database.R </b><br /> <b>2008 panel - download and create database.R</b><br /> <ul> <li>since some variables are <a href=\"http://stat.ethz.ch/R-manual/R-patched/library/base/html/character.html\">character strings</a> in one file and <a href=\"http://stat.ethz.ch/R-manual/R-patched/library/base/html/integer.html\">integers</a> in anoth\ner, initiate <a href=\"http://www.statmethods.net/management/userfunctions.html\">an r function</a> to harmonize <a href=\"http://stat.ethz.ch/R-manual/R-devel/library/base/html/class.html\">variable class</a> inconsistencies in <a href=\"http://smpbff2.dsd.census.gov/pub/sipp/2008/l08puw1.sas\">the sas importation scripts</a> </li> <li>properly handle the parentheses seen in a few of <a href=\"http://smpbff2.dsd.census.gov/pub/sipp/2008/lrw08_xx.sas\">the sas importation scripts</a>, because <a href=\"h\nttp://blog.revolutionanalytics.com/2012/07/importing-public-data-with-sas-instructions-into-r.html\">the SAScii package\n</a> currently does not</li> <li>create an <a href=\"http://cran.r-project.org/web/packages/RSQLite/RSQLite.pdf\">rsqlite database</a>, initiate <a href=\"https://github.com/ajdamico/usgsd/blob/master/SQLite/read.SAScii.sqlite.R\">a variant of the `read.SAScii` function</a> that imports ascii data directly into a sql database (.db)</li> <li>download each microdata file - weights, topical modules, everything - then read 'em into sql</li> </ul> <br /> <b>2008 panel - full year analysis examples.R</b><\nbr /> <ul> <li>define which waves and specific variables to pull into ram, based on the year chosen</li> <li><a href=\"http://www.screenr.com/WgH8\">loop through</a> each of twelve months, constructing a single-year <a href=\"http://www.sqlite.org/lang_createtable.html\">temporary table</a> inside the database</li> <li>read that twelve-month file into working memory, then save it for faster loading later if you like</li> <li>read the main and replicate weights columns into working memory too, <a hre\nf=\"http://stat.ethz.ch/R-manual/R-patched/library/base/html/merge.html\">merge\n</a> everything</li> <li>construct a few annualized and demographic columns using all twelve months' worth of information</li> <li>construct a <a href=\"http://faculty.washington.edu/tlumley/survey/html/svrepdesign.html\">replicate-weighted complex sample design</a> with a <a href=\"http://www.amstat.org/sections/srms/proceedings/y2010/Files/307611_58945.pdf\">fay's adjustment</a> factor of one-half, again save it for faster loading later, only if you're so inclined</li> <li>reproduce census-publish\ned statistics, not precisely (due to topcoding <a href=\"http://www.census.gov/sipp/usrguide/chap4rev2009.pdf\">described here on pdf page 19</a>)</li> </ul> <br /> <b>2008 panel - point-in-time analysis examples.R</b><br /> <ul> <li>define which wave(s) and specific variables to pull into ram, based on the calendar month chosen</li> <li>read that interview point (srefmon)- or calendar month (rhcalmn)-based file into working memory</li> <li>read the topical module and replicate weights files into\nworking memory too, <a href=\"http://stat.ethz.ch/R-manual/R-patched/library/base/html/merge.html\">merge</a> it like you mean it</li> <li>construct a few new, exciting variables using both core and topical module questions</li> <li>construct a <a href=\"http://faculty.washington.edu/tlumley/survey/html/svrepdesign.html\">replicate-weighted complex sample design</a> with a <a href=\"http://www.amstat.org/sections/srms/proceedings/y2010/Files/307611_58945.pdf\">fay's adjustment</a> factor of one-half</\nli> <li>reproduce census-published statistics, not exactly cuz the authors of this brief used the generalized variance formula (gvf) to calculate the margin of error - <a href=\"https://www.census.gov/sipp/usrguide/chap7rev2008.pdf\">see pdf page 4 for more detail</a> - the friendly statisticians at census recommend using the replicate weights whenever possible.  oh hayy, now it is.</li> </ul> <br /> <b>2008 panel - median value of household assets.R</b><br /> <ul> <li>define which wave(s) and spe\ncific variables to pull into ram, based on the topical module chosen\n</li> <li>read the topical module and replicate weights files into working memory too, <a href=\"http://stat.ethz.ch/R-manual/R-patched/library/base/html/merge.html\">merge</a> once again</li> <li>construct a <a href=\"http://faculty.washington.edu/tlumley/survey/html/svrepdesign.html\">replicate-weighted complex sample design</a> with a <a href=\"http://www.amstat.org/sections/srms/proceedings/y2010/Files/307611_58945.pdf\">fay's adjustment</a> factor of one-half</li> <li>reproduce census-published s\ntatistics, not exactly due to topcoding (read more about topcoding by searching\n<a href=\"http://www.census.gov/sipp/usrguide/chap11rev2008.pdf\">this</a> and <a href=\"http://www.census.gov/sipp/usrguide/ch10-rev2009.pdf\">that</a> user guide for, well, `topcoding`).  huh.  so topcoding affects asset statistics.</li> </ul> <br /> <b>replicate census poverty statistics.R</b><br /> <ul> <li>load a single wave of data </li> <li>limit the table to the variables needed for an example analysis </li> <li>construct the <a href=\"http://faculty.washington.edu/tlumley/survey/html/svrepde\nsign.html\">complex sample survey object </a></li> <li>print statistics and standard errors matching the <a href=\"https://github.com/ajdamico/usgsd/blob/master/Survey%20of%20Income%20and%20Program%20Participation/SIPP%20PUF%20Poverty%20Statistics%20from%20Census.pdf?raw=true\">target replication table</a></li> </ul> <br /> <br /> <a href=\"https://github.com/ajdamico/usgsd/tree/master/Survey%20of%20Income%20and%20Program%20Participation\">click here to view these eight scripts</a><br /> <br /> <br /\n> <br /> for more detail about the survey of income and program participation (sipp), visit:<br /> <ul> <li><a href=\"http://www.nap.edu/catalog.php?record_id=12715#toc\">the reengineering of</a> the survey of income and program participation, snore.</li> <li>the survey of income and program participation <a href=\"http://en.wikipedia.org/wiki/Survey_of_Income_and_Program_Participation\">wikipedia entry</a></li> </ul> <br /> <br /> notes:<br /> <br /> sipp is right in the middle of an ultra-long pan\nel, these scripts will update as new files are released.<br /> <br /> don't let the <a href=\"http://www.census.gov/sipp/\">deprecated-looking homepage</a> dissuade you.  the survey of income and program participation is happening now, red-hot.  everything you need is available, albeit somewhat hidden.  there's <a href=\"http://www.census.gov/sipp/intro.html\">a short introduction</a>, <a href=\"http://www.census.gov/sipp/DEWS/2008Schedule.pdf\">the data release schedule</a>, <a href=\"http://thedatawe\nb.rm.census.gov/ftp/sipp_ftp.html\">an official ftp site</a> - with <a href=\"http://smpbff2.dsd.census.gov/pub/sipp/2008/l08puw1d.txt\">codebooks</a> - <a href=\"http://www.census.gov/sipp/notes.html\">advanced user notes</a>, <a href=\"http://www.census.gov/sipp/pubs.html\">census publications based on sipp</a> - don't miss <a href=\"http://www.census.gov/sipp/tables/index.html\">the table packages</a> - aww cool even <a href=\"http://www.census.gov/sipp/content.html\">questionnaires</a>.  the <a href=\"h\nttp://smpbff2.dsd.census.gov/pub/sipp/2008/l08puw1d.txt\">core variable codebook\n</a>  might not win any beauty pageants, but it'd be a wise use of time to  slowly scroll through the first fifty variables.  interview months take  place after `srefmon == 4` and actual times of month and year can be  determined with the `rhcalmn` + `rhcalyr` variables.<br /> <br /> perhaps more than any of the other data sets on this website, working with sipp will get more comfortable as you <a href=\"http://www.pricewatch.com/system_memory/\">increase your ram</a>.  so long as you manipulate t\nhese files with <a href=\"http://www.w3schools.com/sql/default.asp\">sql commands</a> inside <a href=\"http://cran.r-project.org/web/packages/RSQLite/index.html\">the sqlite database (.db)</a> that my automated-download scripts create, you'll process these data line-by-line and therefore be untethered from any computer hardware limits.  but the moment a <a href=\"http://rss.acs.unt.edu/Rdoc/library/DBI/html/dbReadTable.html\">dbReadTable</a> or <a href=\"http://svitsrv25.epfl.ch/R-doc/library/DBI/html/\ndbSendQuery.html\">dbGetQuery</a> command pulls something into working memory, you'll begin gobbling up those precious four, eight, or sixteen gigabytes on your local computer.  in practice, this simply requires that you to define the columns you need at the start, then limit what gets read-in to only those variables.  you'll see it done in my scripts.  if you don't copy that strategy -fair warning- you may hit <a href=\"http://stat.ethz.ch/R-manual/R-patched/library/base/html/Memory-limits.html\">\nallocation errors</a>.  maybe keep the performance tab of your <a href=\"https://www.google.com/search?q=windows%20task%20manager%20performance\">windows task manager</a> handy and <a href=\"http://stat.ethz.ch/R-manual/R-patched/library/base/html/gc.html\">take out the trash</a>.<br /> <br /> <br /> confidential to sas, spss, stata, and sudaan users: <a href=\"https://vimeo.com/52999628\">watch this</a>.  time to  transition to r.  :D"}}]},{"typeName":"producer","multiple":true,"typeClass":"compound","value":[{"producerName":{"typeName":"producerName","multiple":false,"typeClass":"primitive","value":"Anthony Damico"},"producerURL":{"typeName":"producerURL","multiple":false,"typeClass":"primitive","value":"http://www.asdfree.com/"}}]},{"typeName":"distributor","multiple":true,"typeClass":"compound","value":[{"distributorName":{"typeName":"distributorName","multiple":false,"typeClass":"primitive","value":"IQSS Dataverse Network"},"distributorURL":{"typeName":"distributorURL","multiple":false,"typeClass":"primitive","value":"http://dvn.iq.harvard.edu"},"distributorLogoURL":{"typeName":"distributorLogoURL","multiple":false,"typeClass":"primitive","value":"http://dvn.iq.harvard.edu/images/iqss-logo-background.png"}}]},{"typeName":"distributionDate","multiple":false,"typeClass":"primitive","value":"2013"},{"typeName":"dateOfDeposit","multiple":false,"typeClass":"primitive","value":"2013"}]}},"files":[]}}}