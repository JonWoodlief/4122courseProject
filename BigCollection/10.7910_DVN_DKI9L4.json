{"status":"OK","data":{"id":60636,"identifier":"DVN/DKI9L4","persistentUrl":"https://doi.org/10.7910/DVN/DKI9L4","protocol":"doi","authority":"10.7910","publisher":"Harvard Dataverse","publicationDate":"2013-05-21","storageIdentifier":"s3://dvn-cloud:1902.1/21505","latestVersion":{"id":62164,"storageIdentifier":"s3://dvn-cloud:1902.1/21505","versionNumber":2,"versionMinorNumber":0,"versionState":"RELEASED","versionNote":"","distributionDate":"2013","productionDate":"Production Date","lastUpdateTime":"2013-05-31T05:39:01Z","releaseTime":"2013-05-31T00:00:00Z","createTime":"2013-05-24T04:07:53Z","license":"CC0","termsOfUse":"CC0 Waiver","metadataBlocks":{"citation":{"displayName":"Citation Metadata","fields":[{"typeName":"title","multiple":false,"typeClass":"primitive","value":"American Community Survey (ACS)"},{"typeName":"author","multiple":true,"typeClass":"compound","value":[{"authorName":{"typeName":"authorName","multiple":false,"typeClass":"primitive","value":"Damico, Anthony"}}]},{"typeName":"datasetContact","multiple":true,"typeClass":"compound","value":[{"datasetContactEmail":{"typeName":"datasetContactEmail","multiple":false,"typeClass":"primitive","value":"ajdamico@gmail.com"}}]},{"typeName":"dsDescription","multiple":true,"typeClass":"compound","value":[{"dsDescriptionValue":{"typeName":"dsDescriptionValue","multiple":false,"typeClass":"primitive","value":"<h3 class=\"post-title entry-title\" itemprop=\"name\"> analyze the american community survey (acs) with r and monetdb </h3> experimental. think of the american community survey (acs) as the united states' census for off-years - the ones that don't end in zero. every year, one percent of all americans respond, making it the largest complex sample administered by the u.s. government (the decennial census has a much broader reach, but since it attempts to contact 100% of the population, it's not a sur\nvey). the acs asks <a href=\"http://www.census.gov/acs/www/guidance_for_data_users/guidance_main/\">how people live</a> and although the <a href=\"http://www.census.gov/acs/www/methodology/questionnaire_archive/\">questionnaire</a> only includes about three hundred questions on <a href=\"http://www.census.gov/acs/www/about_the_survey/american_community_survey/\">demography, income, insurance</a>, it's often accurate at sub-state geographies and - <a href=\"http://www.census.gov/acs/www/guidance_for_dat\na_users/estimates/\">depending how many years pooled</a> - down to small counties. households are the sampling unit, and once a household gets selected for inclusion, all of its residents respond to the survey. this allows household-level data (like home ownership) to be collected more efficiently and lets researchers examine family structure. the census bureau runs and finances this behemoth, of course.<br /> <br /> the <a href=\"http://www.census.gov/acs/www/data_documentation/data_via_ftp/\">dow\nnloadable american community survey</a> ships as two distinct household-level and person-level comma-separated value (.csv) files. <a href=\"http://stat.ethz.ch/R-manual/R-patched/library/base/html/merge.html\">merging</a> the two just <a href=\"http://cps.ipums.org/cps-action/faq#ques11\">rectangulates</a> the data, since each person in the person-file has exactly one matching record in the household-file. for analyses of small, smaller, and microscopic geographic areas, choose one-, three-, or fiv\ne-year pooled files. use as few pooled years as you can, unless you like sentences that start with, \"over the period of 2006 - 2010, the average american ... [insert yer findings here].\"\n<br /> <br /> rather than processing the acs <a href=\"http://www.census.gov/acs/www/data_documentation/public_use_microdata_sample/\">public use microdata sample</a> line-by-line, the r language brazenly reads everything into memory by default. to prevent overloading your computer, <a href=\"http://www.stat.auckland.ac.nz/showperson?firstname=Thomas&amp;surname=Lumley\">dr. thomas lumley</a> wrote <a href=\"http://sqlsurvey.r-forge.r-project.org/\">the sqlsurvey package</a> principally to deal with t\nhis ram-gobbling monster. if you're already familiar with syntax used for\n<a href=\"http://faculty.washington.edu/tlumley/survey/\">the survey package</a>, be patient and read the sqlsurvey examples carefully when something doesn't behave as you expect it to - some sqlsurvey commands require a different structure (i.e. <a href=\"http://faculty.washington.edu/tlumley/survey/html/svyby.html\">svyby</a> gets called through <a href=\"http://faculty.washington.edu/tlumley/survey/html/surveysummary.html\">svymean</a>) and others might not exist anytime soon (like <a href=\"http://\nfaculty.washington.edu/tlumley/survey/html/svyolr.html\">svyolr</a>). gimme some good news: sqlsurvey uses ultra-fast monetdb (<a href=\"http://www.asdfree.com/2013/03/column-store-r-or-how-i-learned-to-stop.html\">click here for speed tests</a>), so follow the <a href=\"https://github.com/ajdamico/usgsd/blob/master/MonetDB/monetdb%20installation%20instructions.R\">monetdb installation instructions</a> before running this acs code. monetdb imports, writes, recodes data slowly, but reads it hyper-fast\n. a magnificent trade-off: data exploration typically requires you to think, send an analysis command, think some more, send another query, repeat. importation scripts (especially the ones i've already written for you) can be left running overnight sans hand-holding.\n<br /> <br /> the acs weights generalize to the whole united states population including <a href=\"http://www.census.gov/acs/www/Downloads/data_documentation/GroupDefinitions/2011GQ_Definitions.pdf\">individuals living in group quarters</a>, but non-residential respondents get an <a href=\"http://www.census.gov/acs/www/Downloads/data_documentation/GroupDefinitions/2011GQ_Definitions.pdf\">abridged questionnaire</a>, so most (not all) analysts exclude records with a <a href=\"http://www.census.gov/acs\n/www/Downloads/data_documentation/pums/Estimates/pums_estimates_11.lst\">relp variable of 16 or 17\n</a> right off the bat. this new github repository contains four scripts:<br /> <br /> <b>2005-2011 - download all microdata.R</b><br /> <ul>   <li>create the batch (.bat) file needed to initiate the monet database in the future</li>   <li>download, unzip, and import each file for every year and size specified by the user</li>   <li>create and save household- and merged/person-level replicate weight complex sample designs</li>   <li>create a well-documented block of code to re-initiate the monet\ndb server in the future<b> </b></li> </ul> <br /> <i>fair </i><i>warning: this full script takes a loooong time. run it friday afternoon, commune    with nature for the weekend, and if you've got a fast processor and speedy internet connection, monday morning it should be ready for    action. otherwise, either download only the years and sizes you need or - if you gotta have 'em all - run it, minimize it, and then don't disturb it for a week.</i><br />   <br />   <b>2011 single-year - analysis e\nxamples.R</b><br />   <ul>   <li>run the well-documented block of code to re-initiate the monetdb server </li>   <li>load the <a href=\"http://astrostatistics.psu.edu/su07/R/html/base/html/load.html\">r data file</a> (.rda) containing the replicate weight designs for the single-year 2011 file</li>   <li>perform the standard repertoire of analysis examples, only this time using <a href=\"http://sqlsurvey.r-forge.r-project.org/\">sqlsurvey</a> functions</li> </ul>   <b>2011 single-year - variable reco\nde example.R</b><br />   <ul>   <li>run the well-documented block of code to re-initiate the monetdb server </li>   <li>copy the single-year 2011 table to maintain the pristine original</li>   <li>add a new age category variable by hand</li>   <li>add a new age category variable systematically</li>   <li>re-create then save the <a href=\"http://sqlsurvey.r-forge.r-project.org/\">sqlsurvey</a> replicate weight complex sample design on this new table</li>   <li>close everything, then load everything\nback up in a fresh instance of r</li>   <li>replicate <a href=\"http://www.census.gov/acs/www/Downloads/data_documentation/pums/Estimates/pums_estimates_11.lst\">a few of the census statistics</a>. no muss, no fuss</li> </ul>   <b>replicate census estimates - 2011.R</b><br />   <ul>   <li>run the well-documented block of code to re-initiate the monetdb server </li>   <li>load the r data file (.rda) containing the replicate weight designs for the single-year 2011 file</li>   <li>match every nation\nwide statistic on <a href=\"http://www.census.gov/acs/www/Downloads/data_documentation/pums/Estimates/pums_estimates_11.lst\">the census bureau's estimates page</a>, using <a href=\"http://sqlsurvey.r-forge.r-project.org/\">sqlsurvey</a> functions</li> </ul>   <br />   <br />   <a href=\"https://github.com/ajdamico/usgsd/tree/master/American%20Community%20Survey\">click here to view these four scripts</a><br />   <br />   <br /> for more detail about the american community survey (acs), visit:<br /> <\nul>   <li>the us census bureau's acs <a href=\"http://www.census.gov/acs/www/\">homepage</a></li>   <li>the american factfinder <a href=\"http://factfinder2.census.gov/\">homepage</a></li>   <li>the american community survey's <a href=\"http://en.wikipedia.org/wiki/American_Community_Survey\">wikipedia page</a> </li>   <li>the census bureau's <a href=\"https://askacs.census.gov/\">acs frequently asked questions page</a></li> </ul> <br /> notes:<br /> <br /> if you're just looking for a couple data point\ns, you ought to give the census bureau's <a href=\"http://factfinder2.census.gov/\">american factfinder</a> a whirl. it's a table creator (<a href=\"http://www.kaiseredu.org/Tutorials-and-Presentations/Conducting-Research-with-Online-Data-Query-Tools.aspx\">click here to watch me blab about table creators</a>), so it's easy-to-use but inflexible. here's a li'l tip: if you run a statistic using american factfinder and then the same statistic using these scripts, they will be close but won't match exa\nctly. <a href=\"https://ask.census.gov/faq.php?id=5000&amp;faqId=911\">it's not a mistake</a>, and both are methodologically correct.<br /> <br /> every now and then, <a href=\"http://www.psc.isr.umich.edu/pscinfoserv/?p=2683\">grumpy lawmakers threaten to defund the acs</a> because, well, it's expensive. use it or lose it.<br /> <br /> since <a href=\"http://www.monetdb.org/Documentation/Manuals/SQLreference/BuiltinTypes\">data types in sql</a> are not as plentiful as they are <a href=\"http://www.sta\ntmethods.net/input/datatypes.html\">in the r language</a>, the definition of a monet database-backed complex design object requires a cutoff be specified between the categorical variables and the linear ones. that cut point gets defined using the check.factors argument in the sqlsurvey() and sqlrepsurvey() function calls. check.factors defaults to ten, but can be raised or lowered as needed. here's how it works:<br /> <ul>   <li>if the column would be a <a href=\"http://stat.ethz.ch/R-manual/R-pat\nched/library/base/html/character.html\">character string</a> or <a href=\"http://stat.ethz.ch/R-manual/R-patched/library/base/html/factor.html\">factor</a> inside an <a href=\"http://www.r-tutor.com/r-introduction/data-frame\">r data frame</a>, the sql database stores it as a <a href=\"http://www.monetdb.org/Documentation/Manuals/SQLreference/BuiltinTypes\">varchar</a> column.</li>   <li>if the column would be <a href=\"http://stat.ethz.ch/R-manual/R-patched/library/base/html/numeric.html\">numeric</a> o\nr <a href=\"http://stat.ethz.ch/R-manual/R-patched/library/base/html/integer.html\">integer</a> in an <a href=\"http://www.r-tutor.com/r-introduction/data-frame\">r data frame</a>, but has fewer than eleven <a href=\"http://stat.ethz.ch/R-manual/R-devel/library/base/html/unique.html\">unique</a> values, the sql database also stores it as a <a href=\"http://www.monetdb.org/Documentation/Manuals/SQLreference/BuiltinTypes\">varchar</a> column.</li>   <li>if the column would be <a href=\"http://stat.ethz.ch/\nR-manual/R-patched/library/base/html/numeric.html\">numeric</a> or <a href=\"http://stat.ethz.ch/R-manual/R-patched/library/base/html/integer.html\">integer</a> in an <a href=\"http://www.r-tutor.com/r-introduction/data-frame\">r data frame</a>, but has at least eleven <a href=\"http://stat.ethz.ch/R-manual/R-devel/library/base/html/unique.html\">unique</a> values, the sql database stores it as a <a href=\"http://www.monetdb.org/Documentation/Manuals/SQLreference/BuiltinTypes\">double</a> (that's sql-spe\nak for numeric).</li> </ul> <br /> unless specified by the question's phrasing, most acs variables should be treated as point-in-time, as opposed to either <i>annualized</i> or <i>ever during the year</i>. this distinction is particularly important for health insurance coverage. think about these three statistics --<br /> <ul>   <li>the number of americans who won't have health insurance at least once during this year</li>   <li>the number of americans without health insurance right now</li>   <\nli>the number of americans who won't ever have health insurance during this year\n</li> </ul> -- the number of americans without health insurance right now is the point-in-time variable, smaller than the <i>at least once</i> number but larger than the <i>ever</i> number.<br /> <br /> although the automated ftp download program for this data    set only retrieves files back as far as 2005, a nationwide version of the   american community survey has been conducted since 2000. i skipped those years for two reasons --<br />   <ul>   <li>the sample size (the true       strength of\nthe modern acs) wasn't very large on the older files (the        2004 and 2011 single-year person-level files are 54mb and 580mb,        respectively). there's no reason to import these files into a monet database.\n</li>   <li>the <a href=\"http://usa.ipums.org/usa/repwt.shtml\">replicate weight</a>ed       design wasn't implemented until 2005, so the creation of a complex        sample survey object isn't possible. if you need to calculate standard        errors for earlier years, you'll have to rely on a pita <a href=\"http://www.census.gov/acs/www/Downloads/library/2010/2010_Fuller_01.pdf\">generalized variance formula</a> instead. evidence: the <a href=\"http://www.census.gov/acs/www/Downloads/data_document\nation/pums/Estimates/pums_estimates_04.lst\">published estimates prior to 2005\n</a> don't include error columns.</li> </ul> -- but if it's critical for you to analyze this early data,    those tables should be small enough to read into memory with <a href=\"http://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html\">read.csv</a>. use <i>wgtp</i> for household-weighted and <i>pwgtp</i> for person-weighted statistics with <a href=\"http://math.furman.edu/~dcs/courses/math47/R/library/Hmisc/html/wtd.stats.html\">wtd.mean and wtd.quantile</a> functions in <a href=\"ht\ntp://cran.r-project.org/web/packages/Hmisc/index.html\">the Hmisc package</a>.<br />   <br /> confidential to sas, spss, stata, sudaan users: the decennial census is enshrined in our constitution. your statistical software isn't. time to transition to r. :D"}}]},{"typeName":"producer","multiple":true,"typeClass":"compound","value":[{"producerName":{"typeName":"producerName","multiple":false,"typeClass":"primitive","value":"Anthony Damico"},"producerURL":{"typeName":"producerURL","multiple":false,"typeClass":"primitive","value":"http://www.asdfree.com/"}}]},{"typeName":"distributor","multiple":true,"typeClass":"compound","value":[{"distributorName":{"typeName":"distributorName","multiple":false,"typeClass":"primitive","value":"IQSS Dataverse Network"},"distributorURL":{"typeName":"distributorURL","multiple":false,"typeClass":"primitive","value":"http://dvn.iq.harvard.edu"},"distributorLogoURL":{"typeName":"distributorLogoURL","multiple":false,"typeClass":"primitive","value":"http://dvn.iq.harvard.edu/images/iqss-logo-background.png"}}]},{"typeName":"distributionDate","multiple":false,"typeClass":"primitive","value":"2013"},{"typeName":"dateOfDeposit","multiple":false,"typeClass":"primitive","value":"2013"}]}},"files":[]}}}